---
title: "Property Sales Price Prediction"
author: Team 9 - Udbhav Kush, Jack McMorrow, Chris Li
execute:
    echo: false
format: html
---
# Research Topic
Predicted the price of property before it sells is incredibly important for everyone involved in the real estate market. Whether its the sellers, buyers, real estate agents, or listing websites like Zillow, figuring out what to sell or buy a home for is incredibly important. Building models to predict these selling prices would be advantageous to everyone on the market.
We are analyzing a from New York City's department of finace, which has a record of every property that was sold from August 2016 to August 2017. This dataset was initially found on [kaggle](https://www.kaggle.com/datasets/new-york-city/nyc-property-sales).
We are going to analyze the different factors present in our dataset to predict the sales price of the building in New York City. The aim of this project is to find relations and patterns of sales price w.r.t. various other factors to have a clear understanding of property price in NYC, and hopefully will help property companies or citizens that has interest in purchasing property at New York to have a prospect for the NY property price trend in the future.

# SMART Questions: 
* What type(s) of location variable(s) (Eg. block, borough, neighborhood) will affect the selling price most? (can answer by modeling)\
* How is the sale price of commercial property different from the sale price of residential property?\
* How is accuracy different if different machine learning models like linear regression, logistic regression, and random forest are applied?\

After exploring the data more, we mainly focused on the last question to build a model to accurately predict sale price.

# Importing Libraries
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

import statsmodels.api as sm
```

# Importing Data

We imported the data about New York properties and checked its basic information

```{python}
data = pd.read_csv("Team9_data.csv")
print(data.shape)
print(data.info())
```
This dataset has 84,548 rows and 22 columns

# Dataset Pre-Processing

Before we do the EDA and build models for this dataset, we need to clean the data first

## Columns Manipulation
### Changing Column's Name

We changed the columns name into lower-case for better visualization and convenience.\ 
```{python}
newcols = ["unnamed", "borough", "neighborhood", "building_class_category", "tax_class_at_present", "block", "lot", "easement", "building_class_at_present", "address", "apartment_number", "zip_code", "residential_units", "commercial_units", "total_units", "land_square_feet", "gross_square_feet", "year_built", "tax_class_at_time_of_sale", "building_class_at_time_of_sale", "sale_price", "sale_date"]
data.columns = newcols
data.info()
```

### Changing column's data type

There are too many columns with inappropriate data types, so we need to convert them to the appropriate data type:

1) We converted land and gross square feet, property sales price, the number of residential, commercial and total units into continous variables. We will also coerce non-numeric values (such as -) to NaN for better recognition.\
2) We converted variables that represent different locations like borough, neighborhood and zip code into categorical variable; we also converted variables represented different building categories, and different building tax categories into categorical variables\

```{python}
numeric = ["residential_units","commercial_units","total_units", "land_square_feet" ,
           "gross_square_feet","sale_price"]
for col in numeric: 
    data[col] = pd.to_numeric(data[col], errors='coerce')

categorical = ["borough","neighborhood",'building_class_category', 'tax_class_at_present',
               'building_class_at_present','zip_code', 'building_class_at_time_of_sale', 'tax_class_at_time_of_sale']
for col in categorical: 
    data[col] = data[col].astype('category')
```

### Creating Some New Features:

We created a new column for percentage of residential units. 
```{python}
data["percent_residential_units"] = data["residential_units"] / data["total_units"]
# Three lines are greater than one - not possible is likely due to discrepencies in the data so we can drop those
data= data.loc[data["percent_residential_units"] != 6]
data= data.loc[data["percent_residential_units"] != 2]
data= data.loc[data["percent_residential_units"] != 1.5]
data['percent_residential_units'].value_counts()
```

We created another new column named 'age' by calculating the difference between the current year and the year it was built.\

```{python}
data = data[data['year_built'] != 0]
data['age'] = 2022 - data['year_built']
```

### Dropping Insignificant Columns
Based on the dataset information, we can see there's an ambiguous column called 'Unnamed' that we don't know what it's used for, and another useless column 'easement' which has no value in its column for our analysis of sales price, so we decided to drop them first.\

```{python}
data_few_cols = data.drop("unnamed", axis=1)
data_few_cols = data_few_cols.drop("easement", axis=1)
```

Because we created a new feature called "age", which is a better representation for the measure of a property's history, so we can drop the column "year_built".

```{python}
data_few_cols = data_few_cols.drop(['year_built'], axis =1)
```

Lastly, we dropped some other columns too which had similar meanings to other columns and no longer needed.\

```{python}
data_few_cols = data_few_cols.drop(['apartment_number','building_class_at_present', 'tax_class_at_present'], axis=1)
```

### Subsetting Out Inappropriate Values

Dropping duplicates:\

```{python}
data_few_cols.drop_duplicates(keep = "last", inplace=True)
```

There were some rows where the number of commercial and residential do not equal the total units, so we can drop these due to the discrepancies.

```{python}
data_few_cols = data_few_cols[data_few_cols['total_units'] == data_few_cols['commercial_units'] + data_few_cols['residential_units']]
```

Removing rows with TOTAL UNITS == 0 and one outlier with 2261 units:\

```{python}
data_few_cols = data_few_cols[(data_few_cols['total_units'] > 0) & (data_few_cols['total_units'] != 2261)]
```

Removing 0 square foot units:\

```{python}
data_few_cols = data_few_cols[data_few_cols["land_square_feet"] != 0]
data_few_cols = data_few_cols[data_few_cols["gross_square_feet"] != 0]
```

## Dropping Null Values

After we finish the column manipulation, convert columns into appropriate data type, drop useless columns and create some new features, then we can drop the null or missing values in the whole dataset.\

```{python}
data_few_cols = data_few_cols.dropna()
data_few_cols.info()
```

# Exploratory Data Analysis (EDA) & Visualization
## Correlation Matrix
```{python}
corr = data_few_cols.corr()
ax = sns.heatmap(corr, cmap = 'Blues')
plt.title("Correlation Matrix")
plt.show()
```

* According to the correlation matrix, we can see gross_square_feet has the strongest positive correlation with sales price.\

## Sales Prices Distribution
Only plotting values less than $10 million due to significant outliers (eg. 2.2 BILLION dollars) just for visualization purposes.
```{python}
plt.hist(data_few_cols.loc[data_few_cols["sale_price"] <= 10000000]["sale_price"], bins = 30, edgecolor = 'black')
plt.xlabel("Sale Price (Tens of millions)")
plt.ylabel("Frequency")
plt.title("Histogram of House Sale Prices")
plt.show()
```

* Most of the properties are selling below 2 million dollars in New York from 2016 to 2017.\

## Different Districts Selling Prices
```{python}
borough_map = {1: "Manhattan", 2: "Bronx", 3: "Brooklyn", 4: "Queens", 5:"Staten Island"}
data_few_cols["borough"] = data_few_cols.borough.map(borough_map)
data_borough = data_few_cols[["borough", "sale_price"]].groupby(["borough"]).mean()
labels = data_borough.index.values
values = data_borough['sale_price'].values
data_borough

plt.bar(labels, values, edgecolor = "black")
# ax.set_xticklabels(("Manhattan", "Bronx", "Brooklyn", "Queens", "Staten Island"))
plt.ylabel("Average Sale Price (Millions)")
plt.xlabel("Borough")
plt.title("Average Sale Price by Borough")
plt.show()
```

* From the plot we can see that Manhattan property price is much higher than the other boroughs, so Manhattan is the most expensive district in New York.\

## Violinplot for price and borough
```{python}
sns.violinplot(x = "borough", y = "sale_price", data = data_few_cols.loc[data_few_cols["sale_price"] <= 10000000])
plt.title("Distribution of Sale Price by Borough")
plt.xlabel("Borough")
plt.ylabel("Sale Price")
plt.show()
```

* From the violinplot, we can see that Manhattan is much skinnier as compared to others but it stretches to higher sales price.\
* Skinny plot represents that there is no specific price range in which Manhattan properties are getting sold.\
* Rest all the boroughs are fatter in lower sales price region.\

## Plotting Square Footage for < 10k and those not equal to zero
```{python}
data_few_cols_gross = data_few_cols.loc[data_few_cols["gross_square_feet"] < 10000]
data_few_cols_gross = data_few_cols_gross.loc[data_few_cols["gross_square_feet"] != 0]
plt.hist(data_few_cols_gross["gross_square_feet"], bins = 30, edgecolor = "black")
plt.xlabel("Gross Square Feet")
plt.ylabel("Frequency")
plt.title("Histogram of Gross Square Footage")
plt.show()
```

* The right skewed histogram indicating there are less values in upper end of the gross_square_feet.\

## Square Footage and Sale Price
```{python}
data_few_cols_gross = data_few_cols_gross.loc[data_few_cols_gross["sale_price"] <= 20000000]
sns.lmplot(x = "gross_square_feet", y = "sale_price", data = data_few_cols_gross, scatter_kws={"alpha": 0.2}, line_kws={'color': 'red'})
plt.title("Square Footage versus Sale Price")
plt.xlabel("Square Footage")
plt.ylabel("Sale Price")
plt.show()
```

* The data points are very scattered are not aligning very well with the straight line.\

## Age and Sale Price
```{python}
sns.lmplot(x = "age", y = "sale_price", data = data_few_cols.loc[data_few_cols["sale_price"] <= 10000000], scatter_kws={'alpha': 0.2}, line_kws={'color': 'red'})
plt.title("Age versus Sale Price")
plt.xlabel("Age (years)")
plt.ylabel("Sale Price")
plt.show()
```

* The data points are very scattered are not aligning very well with the straight line.\

## Total Units and Sale Price
```{python}
data_low = data_few_cols.loc[data_few_cols["sale_price"] <= 10000000]
sns.lmplot(x = "total_units", y = "sale_price", data = data_low.loc[data_low['total_units'] < 500])
plt.ylim((0, 10000000))
plt.show()
```

* We can see that there is a linear relation of total_units but it is not very clear as there are a lot of outliers.\

## Selling Price at Different Tax Classes
```{python}
tax_class = data_few_cols[["sale_price", "tax_class_at_time_of_sale"]].groupby(["tax_class_at_time_of_sale"]).mean()
labels = tax_class.index.values.astype(str)
values = tax_class.sale_price.values
tax_class

plt.bar(labels, values)
plt.title("Sale Price by Tax Class")
plt.xlabel("Tax Class")
plt.ylabel("Sale Price")
plt.show()
```

* This plot gives valuable insights with respect to different tax classes. Properties with tax class 4 has the highest selling price.\

## Distribution of Percent Residential Units
Not very informative but interesting as most buildings are 100% residential.\

```{python}
plt.hist(data_few_cols["percent_residential_units"], bins = 40, edgecolor = 'black')
plt.title("Histogram of Percent of Residential Units")
plt.xlabel("Percent Residential")
plt.ylabel("Frequency")
plt.show()
```

# Model Building
We are going to build 4 models for predicting the sales_price of New York property, which are:\

* Linear Regression\
* Decision Tree\
* Random Forest\
* K-NN\
The variables we are going to use are:\
borough, building_class_category, zip_codes, total_units, percent_residential_units, age, gross_square_feet, tax_class_at_time_of_sale, building_class_at_time_of_sale, and sale_price as the response variable.\
The variables we don't use:\
We didn't use Neighborhood, block, lot and address because they have the similar information as zip_codes and borough, and we think zip_codes and borough can provide better information on location.\
We didn't use land_square_feet because we decided to use gross_sqaure_feet as a better variable for measuring the area.\

## Removing Outliers
Because in the EDA Section, we've already seen our response variable sale_price is highly right skewed, so before we build the models, we have to remove the outliers or the model will be meaningless.\
We defined a function for detecting outliers, of which setting our outliers quantile below 25% and over 75%.\

```{python}
outliers_indexes = []
def outliers (df, ft):
    q1=df[ft].quantile(0.25)
    q3=df[ft].quantile(0.75)
    iqr=q3-q1
    
    lower_bound=q1-1.5*iqr
    upper_bound=q3+1.5*iqr
    
    outliers=df.index[ (df[ft]<lower_bound) | (df[ft]>upper_bound)]
    return outliers

outliers_indexes.extend(outliers(data_few_cols, 'sale_price'))
print('Number of outliers:', len(outliers_indexes))
```

Define another function for removing outliers for dataframe.\

```{python}
def outliers_remove(df, list):
    list = sorted(set(list)) 
    df_clean = df.drop(list)
    return df_clean

clean_df = outliers_remove(data_few_cols, outliers_indexes)
clean_df = clean_df.loc[clean_df["sale_price"] > 10]
print('Shape of our final cleaned dataframe is:', clean_df.shape)
print('Basic info of our final cleand dataframe:\n', clean_df.info())
```

Check the distribution of the sale price after teh outliers are removed.\

```{python}
plt.hist(clean_df.sale_price, bins = 30, edgecolor = "black")
plt.title("Histogram of Sale Prices")
plt.xlabel("Sale Price (Millions)")
plt.ylabel("Frequency")
plt.show()
```

Much cleaner plot, although not exactly normal distribution but very close to normal distribution.\

## Train/Test Split

Before we starting to build the models, we need to split our cleaned dataframe into training set and testing set.\
Also we transformed categorical variables using onehotencoding and scaled the variables using fit_transform() function.\
```{python}
data_sold_features = data_few_cols[["borough", "building_class_category", "zip_code", "total_units", "percent_residential_units", "age", "gross_square_feet", "tax_class_at_time_of_sale", "building_class_at_time_of_sale", "sale_price"]]
dataPreprocessor = ColumnTransformer( transformers=
    [
        ("categorical", OneHotEncoder(), ["building_class_category", "tax_class_at_time_of_sale", 
                                          "building_class_at_time_of_sale", "borough", "zip_code"
                                          ])
        #("numeric", StandardScaler(), ["total_units", "age", "sale_price", "gross_square_feet", "percent_residential_units"])
    ], verbose_feature_names_out=False, remainder="passthrough",
)

data_sold_features_newmatrix = dataPreprocessor.fit_transform(data_sold_features)
newcolnames = dataPreprocessor.get_feature_names_out()
data_sold_features_new = pd.DataFrame( data_sold_features_newmatrix.toarray(), columns= newcolnames )


X = data_sold_features_new.drop(["sale_price"], axis=1)
y = data_sold_features_new['sale_price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=55)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```

## Linear Regression

We build the linear regression with the regressors we mentioned above at first, then we got a fitting result with 48% R-Squared value, which is not a really bad score. However, we want to make sure this is a useful model (predicting useful results), so we also need to check the model assumptions.\

* **Normally shaped response**: Because we defined two functions to remove the outliers for our response variable sale_price in the pre-processing section and checked its distribution on the EDA section, which is approximately normally distributed so we don't have the problem of skewed response.

* **Multicollinearity**: We checked the VIF values for our regressors, and we found zip_code and percentage_residential_units have extremly high VIF values (about 176 and 89 subsequently), which means we have a serious problem of multicollinearity and this can make our model prediction useless, so we removed these two regressors and checked the VIF values again and finally get every VIF values in an acceptable range (below 10).

* **Linearity**: We checked the scatter plot between the response variable sale_price and three continuous regressors in our model structure, which are total_units, Age, and  gross_square_feet. We found a clear linear relationship between property's sales price with total units and square footage, however, we can't see a clear linear relation between property's sales price and property's age. First, we decided to romove regressor Age, but this can't increase other evaluation values, instead, it dropped our R-Squared values about 2% down, so finally we decided to keep it
After we finish these checkings we build our model again with the remaining regressors: age, total_units, gross_square_feet, borough, tax_class_at_time_of_sale, building_class_at_time_of_sale, and also added two more interaction terms.

```{python}
import scipy
from scipy import stats
from statsmodels.formula.api import ols
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Final model structure 
lm_final = ols(formula=' sale_price ~  age + total_units + gross_square_feet + C(borough) + C(tax_class_at_time_of_sale) + C(building_class_category) + age:C(building_class_category) + total_units:C(building_class_category)', data=clean_df).fit()

# Residual Plot
ypred = lm_final.predict(clean_df)
plt.plot(ypred, lm_final.resid, 'o')
plt.axhline(y=0, color = 'red')
plt.title("Plot of Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()
```

The residual plots for our final model looks acceptable.\

```{python}
print("R_squared value:",lm_final.rsquared)
print("adjusted R_squared value:",lm_final.rsquared_adj)
print("Summary Report: \n", lm_final.summary())
```

The final linear model R-Squared value has dropped to 19.8%, which is definitely not acceptable for predicting a marketing continuous data like property sales price. Moreover, we can't see the AIC and BIC change a lot compare with our previous model, which is also extremely high.\
Thus, finally we concluded that:\

* Though we have some significant coefficients in our fitting summary, the response variable sale_price can be hardly explained with this model structure by using Linear Regression, and we will keep trying other types of models.\ 

## Decision Tree
```{python}
train = []
test = []
depths = list(range(10,101,10))
for depth in depths:
    dt = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=1)
    dt.fit(X_train, y_train)
    train.append(dt.score(X_train, y_train))
    test.append(dt.score(X_test, y_test))
    
plt.plot(depths, train, c = "blue")
plt.plot(depths, test, c = "red")
plt.title("Accuracies at Different Tree Depths")
plt.xlabel("Max Depth")
plt.ylabel("R-Squared")
plt.show()
```

* On checking R squared value for different depths of tree, accuracy fluctuated a lot and not giving a pattern. So no judgement can given by the decision tree regressor for our dataset.

## Random Forest
Random forest being a very strong modeling technique, gave better results than the previous models.\
We tried to run random forest using different permutations and combinations of hyperparameters.\
The best score model is as following\
```{python}
rf = RandomForestRegressor(max_depth=10, max_features=10,random_state = 10)
rf.fit(X_train,y_train)

print("Training score:",rf.score(X_train, y_train))
print("Testing score:",rf.score(X_test, y_test))
```

We got the score of around 29% which is not good enough for sales price prediction but it is better than the previous techniques.\

### Plotting Variable Importance Plot

```{python}
import numpy as np
 
importances = rf.feature_importances_
number_to_keep = 10
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)
indices = np.argsort(importances)[::-1][:number_to_keep]

plt.figure(figsize=(20,10))
plt.title("Feature importances")
plt.bar(range(len(indices)), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(len(indices)), X_train.columns[indices], rotation=30, size=15)
plt.xlim([-1, len(indices)])
plt.show()
```

* From the feature importance plot, we can see that as per the random forest model, gross_square_feet, particular zip code classes and total_units have a very strong impact on the overall score.

## K-Nearest Neighbor
As per professor's feedback on our presentation, we tried K-Nearest Neighbor algorithm.\
Since, real estate brokers, agents, and people who want to buy a property knowingly or unknowingly follow K-NN approach to find the best property for them.\
So, using this approach for sales price prediction models makes sense to follow to logic of these people in the real estate market.\
```{python}
from sklearn.neighbors import KNeighborsRegressor

acc_test = []
acc_train = []
for k in range(4, 35):
    knn_model = KNeighborsRegressor(k).fit(X_train, y_train)
    score_knn_test = knn_model.score(X_test, y_test)
    acc_test.append(score_knn_test)
    #print(k)
    #print("test score:",score_knn_test)
    score_knn_train = knn_model.score(X_train, y_train)
    acc_train.append(score_knn_train)
    #print("train Score", score_knn_train)
    #print("\n")
```
```{python}
plt.plot(np.arange(4,35), acc_test, label="Testing accuracy")
plt.title("Score vs K value plot for test dataset")
plt.xlabel("K value")
plt.ylabel("score")
```

* From the plot, R-squared score is negative initially and increasing with increasing K value at around 35% score, score values is getting saturated.\
* Best test score is achieved at K value of 23.\

```{python}
knn_model = KNeighborsRegressor(23).fit(X_train, y_train)
score_knn = knn_model.score(X_test, y_test)
print("Score at k = 23: ",score_knn)
```

* We got the maximum score of around 37% at the k value of 23.

# CONCLUSION
This project has shown various aspects of predicting the sale price of real estate in New York City. Overall, it proved difficult for us to develop a strong and reliable model to predict sale price. We encountered many challenges in our data, such as missing data points and skewed data. However, we managed to make the following conclusions:

* Sales price was highly dependent on location type variables like boroughs.\
* Sales price was highly dependent on the gross_square_feet of the property.\
* Sales price was highly dependent on the tax class of the property.\
* Different models like linear regression, decision tree, random forest, and KNN gave different R squared score.\
* K-Nearest Neighbor performed best on our dataset with a score of 37%.\

It is important to note that this data is only from a one-year period about five years ago. It would be interesting to see how these models would work using more recent data. The New York City real estate market is very lucrative, especially since the COVID-19 pandemic. Seeing how this has changed would be interesting to investigate. Finally, this data is only for New York City, and our results would likely vary if we used similar methods on data from other cities, such as DC. \
Despite the challenges we encountered, our analysis provided us with interesting information about the real estate market in New York City, which allowed us to gain insight into what factors affect the market the most.\