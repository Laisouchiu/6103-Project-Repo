---
title: "Team09 6103_Project"
format: html
---
# Research Topic
We are analyzing a selling record dataset from the New York City property market, which includes annual selling records of single buildings or unit buildings from 2016 to 2017. 
We are going to analyze the different factors present in our dataset to predict the sales price of the building in New York City. The aim of this project is to find relations and patterns of sales price w.r.t. various other factors to have a clear understanding of property price in NYC, and hopefully will help property companies or citizens that has interest in purchasing propert at New York to have a prospect for the NY property price trend in the future.

# SMART Questions: 
* What type(s) of location variable(s) (Eg. block, borough, neighborhood) will affect the selling price most? (can answer by modeling)
* How is the sale price of commercial property different from the sale price of residential property?
* How is accuracy different if different machine learning models like linear regression, logistic regression, and random forest are applied?

# Importing Libraries
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

import statsmodels.api as sm
```



# Importing Data
We imported the data about New York properties and checked its basic information 
```{python}
data = pd.read_csv("Team9_data.csv")
print(data.shape)
print(data.info())
```
This dataset has 84,548 rows and 22 columns



# Dataset Pre-Processing
Before we do the EDA and build models for this dataset, we need to clean the data first
## Columns manipulation
### Change column's name
We changed the columns name into lower capital for better visualization. 
```{python}
newcols = ["unnamed", "borough", "neighborhood", "building_class_category", "tax_class_at_present", "block", "lot", "easement", "building_class_at_present", "address", "apartment_number", "zip_code", "residential_units", "commercial_units", "total_units", "land_square_feet", "gross_square_feet", "year_built", "tax_class_at_time_of_sale", "building_class_at_time_of_sale", "sale_price", "sale_date"]
data.columns = newcols
data.info()
```
### Changing column's data type
There are too many columns with unappropriate data type, so we need to convert them to the appropriate data type:
1). We converted land and gross square feet, property sales price, the number of residential, commerical and total units into continous variable
2). We converted variables that represent different locations like borough, neighborhood and zip code into categorical variable; we also converted variables represented different building categories, and different building tax categories into categorical variable
```{python}
numeric = ["residential_units","commercial_units","total_units", "land_square_feet" ,
           "gross_square_feet","sale_price" ]
for col in numeric: 
    data[col] = pd.to_numeric(data[col], errors='coerce')

categorical = ["borough","neighborhood",'building_class_category', 'tax_class_at_present',
               'building_class_at_present','zip_code', 'building_class_at_time_of_sale', 'tax_class_at_time_of_sale']
for col in categorical: 
    data[col] = data[col].astype('category')
```
### Creating some new features:
We created a new column for percentage of residential units 
```{python}
data["percent_residential_units"] = data["residential_units"] / data["total_units"]
# Three lines are greater than one - not possible is likely due to discrepencies in the data so we can drop those
data= data.loc[data["percent_residential_units"] != 6]
data= data.loc[data["percent_residential_units"] != 2]
data= data.loc[data["percent_residential_units"] != 1.5]
data['percent_residential_units'].value_counts()
```
We created another new column named 'age' by calcualting the difference between the current year and the year it built
```{python}
data = data[data['year_built'] != 0]
data['age'] = 2022 - data['year_built']
```
### Drop useless columns
Based on the dataset information, we can see there's an ambiguous column called 'Unamed' that we don't know what it's describe for, and another useless column 'easement' for our analysis for sales price, so we decided to drop them first
```{python}
data_few_cols = data.drop("unnamed", axis=1)
data_few_cols = data_few_cols.drop("easement", axis=1)
```
Because we created a new feature called "age", which is a better representation for the measure of a property's history, so we can delete the column 'year_built'
```{python}
data_few_cols = data_few_cols.drop(['year_built'], axis =1)
```
Lastly, we dropped some useless columns
```{python}
data_few_cols = data_few_cols.drop(['apartment_number','building_class_at_present', 'tax_class_at_present'], axis=1)
```
### Subseting out inappropriate values
Dropping duplicates
```{python}
data_few_cols.drop_duplicates(keep = "last", inplace=True)
```
Removing data where commercial + residential doesn't equal total units
```{python}
data_few_cols = data_few_cols[data_few_cols['total_units'] == data_few_cols['commercial_units'] + data_few_cols['residential_units']]
```
Removing rows with TOTAL UNITS == 0 and one outlier with 2261 units
```{python}
data_few_cols = data_few_cols[(data_few_cols['total_units'] > 0) & (data_few_cols['total_units'] != 2261)]
```
Removing 0 sqaure foot units
```{python}
data_few_cols = data_few_cols[data_few_cols["land_square_feet"] != 0]
data_few_cols = data_few_cols[data_few_cols["gross_square_feet"] != 0]
```
## Dropping Null Values
After we finish the column manipulation, convert columns into appropriate data type, drop useless columns and create some new features, then we can drop the null or missing values in the whole dataset. 
```{python}
data_few_cols = data_few_cols.dropna()
```
## Removing Outliers
Define a function for detecting outliers, of which setting our outliers quantile below 25% and over 75%
```{python}
outliers_indexes = []
def outliers (df, ft):
    q1=df[ft].quantile(0.25)
    q3=df[ft].quantile(0.75)
    iqr=q3-q1
    
    lower_bound=q1-1.5*iqr
    upper_bound=q3+1.5*iqr
    
    outliers=df.index[ (df[ft]<lower_bound) | (df[ft]>upper_bound)]
    return outliers

outliers_indexes.extend(outliers(data_few_cols, 'sale_price'))
print('Number of outliers:', len(outliers_indexes))
```
Define another function for removing ourliers for dataframe
```{python}
def outliers_remove(df, list):
    list = sorted(set(list)) 
    df_clean = df.drop(list)
    return df_clean

clean_df = outliers_remove(data_few_cols, outliers_indexes)
clean_df = clean_df.loc[clean_df["sale_price"] > 10]
print('Shape of our final cleaned dataframe is:', clean_df.shape)
print('Basic info of our final cleand dataframe:\n', clean_df.info())
```



# Exploratory Data Analysis (EDA)
## Plot Sales Price After Outliers Removed
```{python}
plt.hist(clean_df.sale_price, bins = 30, edgecolor = "black")
plt.title("Histogram of Sale Prices")
plt.xlabel("Sale Price (Millions)")
plt.ylabel("Frequency")
plt.show()
```
## Correlation Matrix
```{python}
corr = clean_df.corr()
ax = sns.heatmap(corr, cmap = 'Blues')
plt.title("Correlation Matrix")
plt.show()
```



# Model Building
We are going to build 4 models for predicting the sales_price of New York property, which are:
* Linear Regression
* Decision Tree
* Random Forest
* K-NN
The variables we are going to use are: 
borough, building_class_category, zip_codes, total_units, percent_residential_units, age, gross_square_feet, tax_class_at_time_of_sale, building_class_at_time_of_sale, and sale_price as the response variable. 
The variables we don't use: 
We didn't use Neighborhood, block, lot and address because they have the similar information as zip_codes and borough, and we think zip_codes and borough can provide better information on location. 
We didn't use land_square_feet because we decided to use gross_sqaure_feet as a better variable for measuring the area.
## Train/Test Split
Before we starting to build the models, we need to split our cleaned dataframe into training set and testing set
```{python}
data_sold_features = data_few_cols[["borough", "building_class_category", "zip_code", "total_units", "percent_residential_units", "age", "gross_square_feet", "tax_class_at_time_of_sale", "building_class_at_time_of_sale", "sale_price"]]
dataPreprocessor = ColumnTransformer( transformers=
    [
        ("categorical", OneHotEncoder(), ["building_class_category", "tax_class_at_time_of_sale", 
                                          "building_class_at_time_of_sale", "borough", "zip_code"
                                          ])
        #("numeric", StandardScaler(), ["total_units", "age", "sale_price", "gross_square_feet", "percent_residential_units"])
    ], verbose_feature_names_out=False, remainder="passthrough",
)

data_sold_features_newmatrix = dataPreprocessor.fit_transform(data_sold_features)
newcolnames = dataPreprocessor.get_feature_names_out()
data_sold_features_new = pd.DataFrame( data_sold_features_newmatrix.toarray(), columns= newcolnames )


X = data_sold_features_new.drop(["sale_price"], axis=1)
y = data_sold_features_new['sale_price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=55)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```
## Linear Regression
We build the linear regression with the regressors we mentioned above at first, then we got a fitting result with 48% R-Squared value, which is not a really bad score. However, we want to make sure this is a useful model (predicting useful results), so we also need to check the model assumptions
* Normally shaped response: Because we defined two functions to remove the outliers for our response variable sale_price in the Pre-processing section and checked its distribution on the EDA section, which is approximately normally distributed so we don't have the problem of skewed response
* Multicollinearity: We checked the VIF values for our regressors, and we found zip_code and percentage_residential_units have extremly high VIF values (about 176 and 89 subsequently), which means we have a serious problem of multicollinearity and this can make our model prediction useless, so we removed these two regressors and checked the VIF values again and finally get every VIF values in an acceptable range (below 10)
* Linearity: We checked the scatter plot between the response variable sale_price and three continuous regressors in our model structure, which are total_units, Age, and  gross_square_feet. We found a clear linear relationship between property's sales price with total units and square footage, however, we can't see a clear linear relation between property's sales price and property's age. Thus firstly we decided to romove regressor Age, but this can't increase other evaluation values, instead, it dropped our R-Squared values about 2% down, so finally we decided to keep it
After we finish these checkings we build our model again with the remaining regressors: age, total_units, gross_square_feet, borough, tax_class_at_time_of_sale, building_class_at_time_of_sale, and also added two more interaction terms 
```{python}
from statsmodels.formula.api import ols
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Final model structure 
lm_final = ols(formula=' sale_price ~  age + total_units + gross_square_feet + C(borough) + C(tax_class_at_time_of_sale) + C(building_class_category) + age:C(building_class_category) + total_units:C(building_class_category)', data=clean_df).fit()

# Residual Plot
ypred = lm_final.predict(clean_df)
plt.plot(ypred, lm_final.resid, 'o')
plt.axhline(y=0, color = 'red')
plt.title("Plot of Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show() 
```
The residual plots for our final model looks acceptable
```{python}
print(lm_final.summary())
```
The final linear model R-Squared value has dropped to 19.8%, which is definetly not acceptable for predicting a marketing continuous data like property sales price. Moreover, we can't see the AIC and BIC change a lot compare with our previous model, which is also extremely high.
Thus, finally we concluded that: 
* Though we have some significant coefficients in our fitting summary, the response variable sale_price can hardly explain with this model structure by using Linear Regression, and we will keep trying other types of models. 
## Decision Tree
```{python}

```
## Random Forest
```{python}

```
## K-NN
```{python}

```
